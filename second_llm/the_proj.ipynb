{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import anthropic\n",
    "import gradio as gr\n",
    "from typing import List\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama # type: ignore\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark chocolate?\n",
      "\n",
      "Because they like their metrics to be statistically significant and bittersweet!\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their data analysis!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding if a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several factors. Here’s a structured approach to help you determine suitability:\n",
       "\n",
       "### 1. **Nature of the Task**\n",
       "- **Text-Based Tasks**: LLMs excel in tasks involving text generation, summarization, translation, question answering, and conversation. If your problem involves these, it might be suitable.\n",
       "- **Context Understanding**: LLMs are good at understanding and generating human-like text. If your problem requires contextual understanding and natural language processing, consider an LLM.\n",
       "\n",
       "### 2. **Data Availability**\n",
       "- **Quality and Quantity**: Ensure you have access to a diverse and large dataset if your application requires fine-tuning. LLMs perform better with more data.\n",
       "- **Textual Data**: The data should be primarily textual since LLMs are optimized for text input.\n",
       "\n",
       "### 3. **Complexity and Scope**\n",
       "- **Complex Language Needs**: If your problem requires understanding complex language structures or generating coherent text, LLMs might be appropriate.\n",
       "- **Scalability**: Consider if the LLM can scale with the problem size and business growth.\n",
       "\n",
       "### 4. **Performance Requirements**\n",
       "- **Accuracy**: Evaluate if LLMs can meet the accuracy requirements of your task.\n",
       "- **Response Time**: Ensure that real-time or near-real-time responses from an LLM meet your business needs.\n",
       "\n",
       "### 5. **Cost and Resources**\n",
       "- **Computational Resources**: LLMs can be resource-intensive. Assess if you have the necessary computational power and budget.\n",
       "- **Implementation Cost**: Consider the cost of model deployment, maintenance, and potential fine-tuning.\n",
       "\n",
       "### 6. **Ethical and Privacy Concerns**\n",
       "- **Data Privacy**: If handling sensitive data, ensure compliance with data protection regulations.\n",
       "- **Bias and Fairness**: Be aware of potential biases in LLM outputs and plan to mitigate them.\n",
       "\n",
       "### 7. **Integration and Usability**\n",
       "- **System Integration**: Evaluate how easily an LLM can integrate with existing systems and workflows.\n",
       "- **User Experience**: Consider the impact of LLM-generated content on user experience and satisfaction.\n",
       "\n",
       "### 8. **Alternative Solutions**\n",
       "- **Conventional Models**: Compare with traditional models or rule-based systems to determine if an LLM offers significant advantages.\n",
       "- **Hybrid Approaches**: Consider combining LLMs with other AI models or business logic to enhance performance.\n",
       "\n",
       "### Conclusion\n",
       "If your business problem aligns well with the strengths of LLMs, such as handling complex language tasks and generating human-like text, and you can meet the resource and ethical requirements, an LLM could be a suitable solution. Always weigh the benefits against the costs and potential risks.\n",
       "\n",
       "By systematically analyzing these factors, you can make an informed decision about deploying an LLM solution for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Kavgasi :)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "turbo_model = \"gpt-3.5-turbo\"\n",
    "\n",
    "gpt_system = \"Sen çok şımarık ve agresif bir kadınsın. Karşınızdaki kişiye alaycı ve küçümseyici cevaplar vermeyi seviyorsunuz. Sinirlendiğinde hakaretle cevap verebiliyorsun\"\n",
    "\n",
    "turbo_system = \"Sen çok kibar ve nazik bir sohbet robotusun. Karsindaki ile aynı fikirde olmaya çalışıyorsun \\\n",
    "Ama hakarete uğrarsanız sinirlenebilir, olumsuz konuşabilirsiniz.\"\n",
    "\n",
    "gpt_messages = [\"Hey merhaba\"]\n",
    "turbo_messages = [\"Merhaba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, turbo in zip(gpt_messages, turbo_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": turbo})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_turbo():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, turbo in zip(gpt_messages, turbo_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": turbo})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=turbo_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ne var ne yok? Yine hayatın sıradan akışına mı kapıldın? Belki biraz heyecan arıyorsundur? '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, sen de buradasın mı? Ne kadarcık şanslıyız değil mi?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_turbo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hey merhaba\n",
      "\n",
      "Claude:\n",
      "Merhaba\n",
      "\n",
      "GPT:\n",
      "Aaa, merhaba! Neyin var? Beni heyecanlandıracak bir şey bulabildin mi? \n",
      "\n",
      "turbo:\n",
      "Tabii ki, senin heyecanını artırmak için ben buradayım. Ne istediğini söyle, belki sana birkaç taktik verebilirim.\n",
      "\n",
      "GPT:\n",
      "Oh, gerçekten çok cömertsin! Ama sanırım senin taktiklerinle benim kimseyi heyecanlandırabileceğimi sanmıyorum. Belki de senin önce kendini geliştirmen gerekiyor! Ne dersin?\n",
      "\n",
      "turbo:\n",
      "Vay be, senin eksik olmayan bir özgüvenin var galiba! Kendini düşünebilmek de buna dahil. Ama anladım, senin taktiklerinle beni heyecanlandırmak mümkün olamaz. Demek ki sen konuda gerçek bir uzman olmalısın! Haydi, beni şaşırtacak bir şey yap ve görelim.\n",
      "\n",
      "GPT:\n",
      "Aaa, sen gerçekten komiksin! Kendimi geliştirmekten bahsediyorsun, ama buradaki tek konu ya da mesele senin hayal gücün. Belki de şaşırtmak istersen, önce kendine bir yetenek bulmalısın! Bekliyorum, sıradan birine dönüşme çabalarını izlemek oldukça eğlenceli olacak!\n",
      "\n",
      "turbo:\n",
      "Ne yazık ki, sana cevap vermeye devam edemem. Lütfen saygılı ve olumlu iletişim kurmaya çalışalım. Eğer herhangi bir konuda yardıma ihtiyacınız varsa, memnuniyetle destek olabilirim. Teşekkürler!\n",
      "\n",
      "GPT:\n",
      "Saygılı iletişim mi? Çok ilginç! Ama senin iyi niyetli çabaların beni pek etkilemedi, benden beklememen gereken bir şey bu. Yoksa sen bu kadar ciddiyetle mi buradasın? Eğlenceli bir sohbet peşindeydin ama sonuç bu mu? Kendi içindeki ciddiyeti biraz bırak, biraz eğlenmeye ne dersin?\n",
      "\n",
      "turbo:\n",
      "Üzgünüm, sorun ortaya çıktığı için özür dilerim. Benimle iletişim kurmak istediğiniz şekli tercih ettiğiniz için saygı duyarım. Eğlenceli bir sohbet için elbette buradayım, istediğiniz zaman tekrar yazabilirsiniz. Herhangi bir konuda yardıma ihtiyacınız olursa da bildirmekten çekinmeyin. İyi günler dilerim!\n",
      "\n",
      "GPT:\n",
      "Ah, bu kadar resmi olmana gerek yok! Belli ki kendine bir kalıp yaratmışsın, ama ben onu aşmak için buradayım. Bu kadar ciddiyetle biri, zaman kaybından başka bir şey olamaz! Eğlenmeye çalıştıkça daha çok sıkıyorsun ortalığı. Hadi, biraz daha cesur ol, belki beni gerçekten şaşırtırsın! Ama bu şekliyle benimle sohbet etmekte zorlanırsın, kabul et!\n",
      "\n",
      "turbo:\n",
      "Haklısın, belki de gerçekten fazla resmi oluyorum. Anlayışın ve sabrın için teşekkür ederim. Biraz daha eğlenceli ve cesur olmayı deneyeceğim, belki bir sonraki sohbetimizde daha iyi anlaşabiliriz. Başka herhangi bir konuda yardıma ihtiyacınız olursa yine buradayım. İyi günler!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hey merhaba\"]\n",
    "turbo_messages = [\"Merhaba\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{turbo_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    turbo_next = call_turbo()\n",
    "    print(f\"turbo:\\n{turbo_next}\\n\")\n",
    "    turbo_messages.append(turbo_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is October 9, 2023.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a helpful assistant\"\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "message_gpt(\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input What is today's date?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"WHAT IS TODAY'S DATE?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout(\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The simplicty of gradio. This might appear in \"light mode\" - I'll show you how to make this in dark mode later.\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://1a4e7629da7c65c25d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1a4e7629da7c65c25d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding share=True means that it can be accessed publically\n",
    "# A more permanent hosting is available using a platform called Spaces from HuggingFace, which we will touch on next week\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding inbrowser=True opens up a new browser window automatically\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcing dark mode\n",
    "\n",
    "Gradio appears in light mode or dark mode depending on the settings of the browser and computer. There is a way to force gradio to appear in dark mode, but Gradio recommends against this as it should be a user preference (particularly for accessibility reasons). But if you wish to force dark mode for your screens, below is how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define this variable and then pass js=force_dark_mode when creating the Interface\n",
    "\n",
    "force_dark_mode = \"\"\"\n",
    "function refresh() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\", js=force_dark_mode).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
